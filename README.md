## Tokenization 

Tokenization is a very critical stage in LLM training or inference process. It allows us to tune the efficieny of implementing various stages of LLMs. Lets explore tokenization in detail. 

There are several popular tokenization techniques such as byte pairing algorithm and sentencepiece. In addition, tools like tiktoken is also widely used for tokenization. 
