## Tokenization 

Tokenization is a very critical stage in LLM training or inference process. Lets explore tokenization in detail. 

There are several popular tokenization techniques such as byte pairing algorithm and sentencepiece. In addition, tools like tiktoken is also widely used for tokenization. 
